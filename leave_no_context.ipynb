{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa1d92e-a293-4703-bad4-b4d13a7d4823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"leave_no_context_behind.pdf\")\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2018533-f121-41fc-aab6-75ecc6960284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=300)\n",
    "\n",
    "chunks = text_splitter.split_documents(doc)\n",
    "\n",
    "print(len(chunks))\n",
    "\n",
    "print(type(chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "100a24a9-a0be-4254-8268-f03e3098a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('.demo_key.txt')\n",
    "\n",
    "OPENAI_API_KEY = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aacf4ac-d7b6-4e3a-803a-3703278284d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db00c30c-435a-4784-9687-4acfe61dd32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "db = Chroma.from_documents(chunks, embedding_model, persist_directory=\"./chroma\")\n",
    "\n",
    "\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "644c1a01-6a55-4150-90ee-c318657bdd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connection = Chroma(persist_directory=\"./chroma\", embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88d6dbce-16ca-4ca7-a643-024cdc929490",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Everything about the paper, Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\"\n",
    "\n",
    "out = db_connection.similarity_search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c79d1c3d-3d38-4c3f-83f4-5a33f01be594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "f = open('.demo_key.txt')\n",
    "\n",
    "open_ai_key = f.read()\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\", api_key=open_ai_key, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c605d89-fd3c-42dd-b051-75f1a6f0965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "Answer the user's questions based on the below context. Give them in clear bullet points if required, for clarity.\n",
    "If the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    "\n",
    "question_answering_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            SYSTEM_TEMPLATE,\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(chat_model, question_answering_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56505e59-09fd-4414-a8e4-3ba871c8a361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the paper, several experiments were conducted to demonstrate the effectiveness of the proposed approach. Here are some of the experiments mentioned:\\n1. The approach was tested on long-context language modeling benchmarks.\\n2. A passkey retrieval task with a 1M sequence length was solved using a 1B LLM.\\n3. A book summarization task with a 500K length was performed using an 8B model.\\n4. The model was trained with a 100K sequence length and showed improved perplexity.\\n5. The model achieved a new state-of-the-art (SOTA) result on a 500K length book summarization task after continual pre-training and task fine-tuning.\\n\\nThese experiments aimed to showcase the performance and scalability of the proposed Infini-attention mechanism in handling infinitely long inputs with bounded memory and computation resources.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "document_chain.invoke(\n",
    "    {\n",
    "        \"context\": out,\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"what experiments were conducted in this paper?\")\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
